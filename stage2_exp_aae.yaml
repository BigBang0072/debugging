description: Testing the job sumbmission via amlt

target:
  # service: amlk8s
  # # run "amlt target list amlk8s" to list the names of available AMLK8s targets
  # name: itpeusp40cl
  # vc: resrchvc

  service: sing
  #run amlt tl sing to list all the names in singularity
  name: msrresrchvc
  workspace_name: msrresrchws

environment:
  image: tensorflow/tensorflow:2.7.1-gpu
  registry: docker.io # any public registry can be specified here
  setup:
    - bash experiment_setup_cad.sh


code:
  # local directory of the code. this will be uploaded to the server.
  # $CONFIG_DIR is expanded to the directory of this config file
  local_dir: $CONFIG_DIR/

data:
  # You need to run "python src/download_data.py" beforehand
  # to generate the dataset to be uploaded
  # don't forget to run with --upload-data
  # local_dir: $CONFIG_DIR/data/

  # The data will be uploaded to your default storage.
  #   Check ``multi_storage.yaml'' for more flexibility.
  # remote_dir: dataset/twitter_pan16_mention_gender/
  remote_dir: dataset/twitter_aae_sentiment_race/
  # remote_dir: dataset/multinli_1.0/


# Training all the configuration as hyperparam sweep
search:
  job_template:
    name: search_{experiment_name:s}_{auto:3s}
    sku: G1
    command:
    - python transformer_debugger.py -expt_num "cad.civils2.rnum({run_num}).topic({topic_name}).sample({sample}).noise({noise}).pval({pval}).t0_ate({t0_ate}).telambda({te_lambda})" -num_sample {sample} -num_topics {num_topics} -num_epochs {mainepoch} -path $$AMLT_DATA_DIR  -out_path $$AMLT_DATA_DIR -emb_path "glove-wiki-gigaword-100" -vocab_path "assets/word2vec_10000_200d_labels.tsv" -max_len {max_len}  -noise_ratio {noise} -num_hidden_layer {hlayer}  -main_model_mode {main_mode} --normalize_emb -lr {lr} -batch_size {batch_size} -dtype {dtype} -loss_type {loss_type} -teloss_type {teloss_type} -cfactuals_bsize {cfactuals_bsize}  -num_pos_sample {pos_size} -num_neg_sample {pos_size} -te_lambda {te_lambda} -run_num {run_num} -dropout_rate {dropout_rate} -t0_ate {t0_ate} -debug_tidx {debug_tidx} -stage_mode {stage_mode} -topic_name {topic_name} -topic_pval {pval} --bert_as_encoder -transformer {transformer} --train_bert
  type: grid
  max_trials: 100000
  params:
    - name: run_num
      values: [0,1,2]
    - name: sample
      values: [10000,]
    - name: batch_size
      values: [32,]
    - name: debug_tidx
      values: [0,]
    - name: noise
      values: [0.0,]
    - name: dtype
      values: ["aae"]
    - name: num_topics
      values: [1,]
    - name: topic_name
      values: ["race"]
    - name: max_len
      values: [80,]
    - name: mainepoch
      values: [20]
    - name: loss_type
      values: ["x_entropy"]
    - name: dropout_rate
      values: ["0.0"]
    - name: main_mode
      values: ["non_causal",]
    - name: cfactuals_bsize
      values: [1,]
    - name: hlayer
      values: [0,]
    - name: stage_mode
      values: ["stage2_te_reg_strong",]
    - name: teloss_type
      values: ["mse",]
    - name: pos_size
      values: [1,]
    - name: te_lambda
      values: [1,5,10]
    - name: pval
      # values: [0.5]
      # values: [0.6]
      # values: [0.7]
      # values: [0.8]
      # values: [0.9]
      values: [0.99]
    - name: t0_ate
      # values: [0.0,-0.07,0.02,-0.0002,0.01,-0.005,0.01,-0.001,0.03,]
      # values: [0.0,0.02,0.09,0.124,0.10,0.08,0.05,0.05,0.03,]
      # values: [0.0,0.13,0.19,0.0006,0.19,0.131,0.15,0.09,0.127,]
      # values: [0.0,0.04,0.18,0.008,0.26,0.21,0.18,0.17,0.197,]
      # values: [0.0,0.25,0.35,0.06,0.04,0.26,0.22,0.25,0.22,]
      values: [0.0,0.14,0.34,0.35,0.31,0.35,0.24,0.27,0.24,]
    - name: transformer
      values: ["bert-base-uncased",]
    - name: lr
      values: ["5e-5",]


#roberta-base      : lr=1e-5
#bert-base-uncased : lr=5e-5