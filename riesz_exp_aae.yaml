description: Testing the job sumbmission via amlt

target:
  # service: amlk8s
  # # run "amlt target list amlk8s" to list the names of available AMLK8s targets
  # name: itpeusp40cl
  # vc: resrchvc

  service: sing
  #run amlt tl sing to list all the names in singularity
  name: msrresrchvc
  workspace_name: msrresrchws

environment:
  image: tensorflow/tensorflow:2.7.1-gpu
  registry: docker.io # any public registry can be specified here
  setup:
    - bash experiment_setup_cad.sh


code:
  # local directory of the code. this will be uploaded to the server.
  # $CONFIG_DIR is expanded to the directory of this config file
  local_dir: $CONFIG_DIR/

data:
  # You need to run "python src/download_data.py" beforehand
  # to generate the dataset to be uploaded
  # don't forget to run with --upload-data
  # local_dir: $CONFIG_DIR/data/

  # The data will be uploaded to your default storage.
  #   Check ``multi_storage.yaml'' for more flexibility.
  # remote_dir: dataset/twitter_pan16_mention_gender/
  remote_dir: dataset/twitter_aae_sentiment_race/
  # remote_dir: dataset/multinli_1.0/
  # remote_dir: dataset/civilcomments/


# Training all the configuration as hyperparam sweep
search:
  job_template:
    name: search_{experiment_name:s}
    sku: G1
    command:
    - python transformer_debugger.py -expt_num "cad.aaes1riesz.rnum({run_num}).sample({sample}).hlayer({hlayer}).pval({pval}).dtidx({debug_tidx}).rr_lmd({rr_lambda}).reg_lmd({reg_lambda}).tmle_lmd({tmle_lambda}).l2_lmd({l2_lambda}).noise({noise}).topic({topic_name}).replace_strategy({replace_strategy}).gmode({gval_select_mode})" -num_sample {sample} -num_topics {num_topics} -num_epochs {mainepoch} -path $$AMLT_DATA_DIR  -out_path $$AMLT_DATA_DIR -emb_path "glove-wiki-gigaword-100" -vocab_path "assets/word2vec_10000_200d_labels.tsv" -noise_ratio {noise} -num_hidden_layer {hlayer}  -main_model_mode {main_mode} --normalize_emb -lr {lr} -dtype {dtype}  -loss_type {loss_type} -cfactuals_bsize {cfactuals_bsize}  -run_num {run_num} -dropout_rate {dropout_rate} -debug_tidx {debug_tidx}  -stage_mode {stage_mode} -rr_lambda {rr_lambda} -tmle_lambda {tmle_lambda} -l2_lambd {l2_lambda} -reg_lambda {reg_lambda} -num_postalpha_layer {num_postalpha_layer} -batch_size {batch_size} -max_len {max_len} -topic_name {topic_name} -topic_pval {pval} --bert_as_encoder -transformer {transformer} --train_bert -riesz_reg_mode {reg_mode} #-best_gval_selection_metric {gval_select_mode} --select_best_gval --select_best_alpha
  type: grid
  max_trials: 100000
  params:
    - name: run_num
      values: [1,11,21] #[0,10,20] [1,11,21] will be series now on
    - name: sample
      values: [10000,]
    - name: batch_size
      values: [32,]
    - name: debug_tidx
      values: [0,]
    - name: noise
      values: [0.0,0.1]
    - name: dtype
      values: ["aae"]
    - name: num_topics
      values: [1,]
    - name: topic_name
      values: ["race"]
    - name: replace_strategy
      values: ["gpt3"]
    - name: max_len
      values: [80,]
    - name: mainepoch
      values: [20]
    - name: loss_type
      values: ["x_entropy"]
    - name: dropout_rate
      values: ["0.0"]
    - name: main_mode
      values: ["non_causal",]
    - name: cfactuals_bsize
      values: [1,]
    - name: hlayer
      values: [0,]
    - name: num_postalpha_layer
      values: [0,]
    - name: rr_lambda
      values: [0,]
    - name: reg_lambda
      values: [1,]
    - name: tmle_lambda
      values: [0,]
    - name: l2_lambda
      values: [0.0,1.0,10.0,100.0,200.0,1000.0]
    - name: stage_mode
      values: ["stage1_riesz",]
    - name: reg_mode
      values: ["mse",]
    - name: gval_select_mode
      values: ["loss"]
    - name: pval
      values: [0.5,0.6,0.7,0.8,0.9,0.99]
    - name: transformer
      values: ["bert-base-uncased",]
    - name: lr
      values: ["5e-5",]


#roberta-base      : lr=1e-5
#bert-base-uncased : lr=5e-5