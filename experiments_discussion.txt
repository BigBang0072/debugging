====================================================
Experiment1
====================================================
    Encoder has low capacity
    Decoder has Dense begining so that intervnetion dont remove half of the image

    Result: It seems, the causal part is removing every information 
            (was a valid solution based on current obj)
            since we just wanted the causal part to be non-discriminative of domain.

            Prolly: putting the ERM constraint on causal part will help here.

====================================================

====================================================
Experiment2
====================================================
    Encoder has low capacity
    Decoder has Dense begining so that intervnetion dont remove half of the image
    Even Discriminator is very low capacity
            This is better choice since then the representation
            will be at max high level, which will make intervnetion
            more meaningful.

    Result: It seems, the causal part is removing every information 
            (was a valid solution based on current obj)
            since we just wanted the causal part to be non-discriminative of domain.

            Yes, now the effect of constant value is even more pronounced
            Atleast they know how to remove everythin.


            Prolly: putting the ERM constraint on causal part will help here.

====================================================


====================================================
Experiment3
====================================================
    Encoder has low capacity
    Decoder has Dense begining so that intervnetion dont remove half of the image
    Even Discriminator is very low capacity
            This is better choice since then the representation
            will be at max high level, which will make intervnetion
            more meaningful.
    Added the Predictor constraint to keep the causal features meaningful

    Result: 
    No difference. The generated image spurious cf has the complete
    image and the generated causal image has none of the actual image,
    but it has some distinction to learn make the classification correct.

    What if we cycle the generated causal counterfactual to be 
    predictive of class.

    Or put the constraint on the generated image.


====================================================

Experiment 4:
Predicotr is now using the images as input, both actual and counterfactual
ones.

The results are more weird. THe training had not converged properly.
And currently we dont see good causal or even spurous generation correctly.


===================================================
Experiment 5:

Lets try very simple case. Given a single digit, can we identify
the 
invariant: digit
spurious/non-invariant : color 

Result:
Same, the causal feature came out to be blank and the non-causal feature 
came out to have all the information. 

===================================================
Experiment 6:

Let try to put dropout in this simple case itself.
So that it tries to diversify its output.


===================================================
Experiment 7:
Need to get adversarial up and running.
keep learning rate slower for generator 
and also keep learning rate slow for discriminator 
if nan is coming 

==================================================
Experiment 8:
Using two different predictor for causal and spurious ones
Accucary ffrom the spurious side remain same, but generation
look better atleast for causal side.
(result seems unstable --> sometime good sometime slightly bad)

Exp 8.1 : Multiply 10* to the -neg spurious loss 
(Bad result)

Exp 8.4: for encoder: 30*enc_pred_loss + reconstruction_loss
8.5 : 80*



=============================================================
====================== NLP DISCUSSION =======================
=============================================================

=============================================
EXPERIMENT 0:

simple BOW. maxlen=100, embedding trainable

result:
trends were good in all the three category
(self selected)

positive ==> ++
negative ==> ++ (importance increased)
neutral ==> --

=============================================

=============================================
EXPERIMENT 1:

maxlen =200
emb_dim = 100
embedding = glove 100 wikipedia -->freezed

result: 
70-71% validation on both single and both

=============================================

=============================================
EXPERIMENT 2:

maxlen =200
emb_dim = 100
embedding = random, trainable

result:
86% validation on both single and both


=============================================

=============================================
EXPERIMENT 3:

maxlen =200
emb_dim = 100
embedding = glove 100 wikipedia --> trainable

result: 
84.9/87(both) validation on both single and both


Looks like Random Train > Glove Train > Glove Freeze
=============================================

Experiment 4:

random, trainable but unit norm.

rest same

Result: 80/82

=============================================




